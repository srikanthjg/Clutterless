{
  "provider": "local",
  "credentials": {
    "endpoint": "http://localhost:11434/v1/chat/completions",
    "apiKey": ""
  },
  "notes": {
    "description": "Ollama local LLM configuration example",
    "what_is_ollama": "Ollama is a tool for running large language models locally on your machine. It provides an OpenAI-compatible API.",
    "installation": {
      "macos": [
        "1. Download Ollama from https://ollama.ai",
        "2. Install the application",
        "3. Ollama will run automatically in the background"
      ],
      "linux": [
        "1. Run: curl -fsSL https://ollama.ai/install.sh | sh",
        "2. Ollama will start as a service"
      ],
      "windows": [
        "1. Download Ollama from https://ollama.ai",
        "2. Run the installer",
        "3. Ollama will run in the system tray"
      ]
    },
    "setup_instructions": [
      "1. Install Ollama (see installation instructions above)",
      "2. Pull a model: ollama pull llama3",
      "3. Verify Ollama is running: curl http://localhost:11434/api/tags",
      "4. The API endpoint is automatically available at http://localhost:11434/v1/chat/completions",
      "5. No API key is required for local Ollama installations"
    ],
    "recommended_models": [
      {
        "name": "llama3",
        "size": "4.7GB",
        "command": "ollama pull llama3",
        "description": "Meta's Llama 3 model, excellent for general tasks"
      },
      {
        "name": "llama3:70b",
        "size": "40GB",
        "command": "ollama pull llama3:70b",
        "description": "Larger Llama 3 model, better quality but requires more resources"
      },
      {
        "name": "mistral",
        "size": "4.1GB",
        "command": "ollama pull mistral",
        "description": "Mistral 7B model, fast and efficient"
      },
      {
        "name": "mixtral",
        "size": "26GB",
        "command": "ollama pull mixtral",
        "description": "Mixtral 8x7B model, high quality responses"
      },
      {
        "name": "phi3",
        "size": "2.3GB",
        "command": "ollama pull phi3",
        "description": "Microsoft's Phi-3 model, lightweight and fast"
      }
    ],
    "system_requirements": {
      "minimum": {
        "ram": "8GB",
        "disk": "10GB free space",
        "note": "For smaller models like phi3 or mistral"
      },
      "recommended": {
        "ram": "16GB or more",
        "disk": "50GB free space",
        "gpu": "Optional but significantly improves performance",
        "note": "For larger models like llama3:70b or mixtral"
      }
    },
    "useful_commands": {
      "list_models": "ollama list",
      "pull_model": "ollama pull <model-name>",
      "remove_model": "ollama rm <model-name>",
      "run_model": "ollama run <model-name>",
      "check_status": "curl http://localhost:11434/api/tags"
    },
    "default_endpoint": "http://localhost:11434/v1/chat/completions",
    "api_key_required": false,
    "advantages": [
      "Complete privacy - data never leaves your machine",
      "No API costs or rate limits",
      "Works offline",
      "Full control over model selection",
      "No external dependencies"
    ],
    "troubleshooting": {
      "connection_refused": [
        "Verify Ollama is running: check system tray (Windows/Mac) or run 'systemctl status ollama' (Linux)",
        "Try restarting Ollama",
        "Check if port 11434 is available: lsof -i :11434"
      ],
      "slow_responses": [
        "Consider using a smaller model (phi3, mistral)",
        "Close other resource-intensive applications",
        "If you have a GPU, ensure Ollama is using it"
      ],
      "model_not_found": [
        "Pull the model first: ollama pull <model-name>",
        "List available models: ollama list",
        "Check model name spelling"
      ]
    },
    "performance_tips": [
      "Use GPU acceleration if available (automatically detected by Ollama)",
      "Start with smaller models (phi3, mistral) and upgrade if needed",
      "Keep Ollama updated for performance improvements",
      "Close unnecessary applications to free up RAM"
    ]
  }
}

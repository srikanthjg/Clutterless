{
  "provider": "local",
  "credentials": {
    "endpoint": "http://localhost:1234/v1/chat/completions",
    "apiKey": ""
  },
  "notes": {
    "description": "LM Studio local LLM configuration example",
    "what_is_lmstudio": "LM Studio is a desktop application for running large language models locally with a user-friendly interface. It provides an OpenAI-compatible API server.",
    "installation": {
      "all_platforms": [
        "1. Download LM Studio from https://lmstudio.ai",
        "2. Install the application for your platform (Windows, macOS, or Linux)",
        "3. Launch LM Studio"
      ]
    },
    "setup_instructions": [
      "1. Install LM Studio (see installation instructions above)",
      "2. Open LM Studio and go to the 'Discover' tab",
      "3. Search for and download a model (e.g., 'llama-3', 'mistral', 'phi-3')",
      "4. Once downloaded, go to the 'Local Server' tab",
      "5. Select your downloaded model from the dropdown",
      "6. Click 'Start Server' to enable the API",
      "7. The API will be available at http://localhost:1234/v1/chat/completions",
      "8. No API key is required by default (can be configured in settings)"
    ],
    "recommended_models": [
      {
        "name": "Meta Llama 3 8B Instruct",
        "size": "~4.7GB",
        "search_term": "llama-3-8b-instruct",
        "description": "Excellent general-purpose model, good balance of quality and speed",
        "quantization": "Q4_K_M recommended for most systems"
      },
      {
        "name": "Mistral 7B Instruct",
        "size": "~4.1GB",
        "search_term": "mistral-7b-instruct",
        "description": "Fast and efficient, great for quick responses",
        "quantization": "Q4_K_M recommended"
      },
      {
        "name": "Phi-3 Mini",
        "size": "~2.3GB",
        "search_term": "phi-3-mini",
        "description": "Lightweight model, perfect for systems with limited resources",
        "quantization": "Q4_K_M recommended"
      },
      {
        "name": "Mixtral 8x7B Instruct",
        "size": "~26GB",
        "search_term": "mixtral-8x7b-instruct",
        "description": "High-quality responses, requires more powerful hardware",
        "quantization": "Q4_K_M or Q3_K_M for lower memory usage"
      }
    ],
    "quantization_guide": {
      "description": "Quantization reduces model size and memory usage with minimal quality loss",
      "recommendations": {
        "Q4_K_M": "Best balance of quality and size (recommended for most users)",
        "Q5_K_M": "Higher quality, larger size",
        "Q3_K_M": "Smaller size, acceptable quality for resource-constrained systems",
        "Q8_0": "Highest quality, largest size, close to original model"
      }
    },
    "system_requirements": {
      "minimum": {
        "ram": "8GB",
        "disk": "10GB free space",
        "cpu": "Modern multi-core processor",
        "note": "For smaller models like Phi-3 or Mistral 7B with Q3/Q4 quantization"
      },
      "recommended": {
        "ram": "16GB or more",
        "disk": "50GB free space",
        "gpu": "NVIDIA GPU with 6GB+ VRAM (optional but highly recommended)",
        "cpu": "Modern multi-core processor",
        "note": "For larger models or better performance"
      },
      "gpu_acceleration": {
        "nvidia": "Automatically detected and used if available (CUDA)",
        "apple_silicon": "Automatically uses Metal acceleration on M1/M2/M3 Macs",
        "amd": "Limited support, check LM Studio documentation"
      }
    },
    "lmstudio_features": {
      "model_discovery": "Browse and download models directly in the app",
      "model_management": "Easy model switching and management",
      "chat_interface": "Test models with built-in chat interface",
      "server_settings": "Configure port, CORS, API keys, and more",
      "performance_monitoring": "View token generation speed and resource usage",
      "prompt_templates": "Customize system prompts and templates"
    },
    "server_configuration": {
      "default_port": 1234,
      "change_port": "Go to Settings > Server > Port",
      "enable_cors": "Enabled by default for browser extensions",
      "api_key": "Optional, can be set in Settings > Server > API Key",
      "context_length": "Configurable per model, affects memory usage"
    },
    "default_endpoint": "http://localhost:1234/v1/chat/completions",
    "api_key_required": false,
    "api_key_note": "API key can be optionally configured in LM Studio settings for security",
    "advantages": [
      "User-friendly graphical interface",
      "Easy model discovery and download",
      "Built-in chat interface for testing",
      "Complete privacy - data never leaves your machine",
      "No API costs or rate limits",
      "Works offline",
      "Excellent GPU acceleration support",
      "Active development and updates"
    ],
    "troubleshooting": {
      "connection_refused": [
        "Verify LM Studio is running and the server is started",
        "Check the 'Local Server' tab in LM Studio",
        "Ensure a model is loaded and the server shows 'Running'",
        "Verify the port number matches (default: 1234)"
      ],
      "slow_responses": [
        "Use a smaller model (Phi-3, Mistral 7B)",
        "Use a more aggressive quantization (Q3_K_M instead of Q4_K_M)",
        "Enable GPU acceleration if available",
        "Reduce context length in server settings",
        "Close other resource-intensive applications"
      ],
      "model_not_loading": [
        "Ensure you have enough RAM for the model",
        "Try a smaller model or more aggressive quantization",
        "Check LM Studio logs for error messages",
        "Restart LM Studio"
      ],
      "server_not_starting": [
        "Check if port 1234 is already in use by another application",
        "Try changing the port in LM Studio settings",
        "Restart LM Studio",
        "Check firewall settings"
      ]
    },
    "performance_tips": [
      "Enable GPU acceleration in settings (automatically detected)",
      "Start with Q4_K_M quantization for best balance",
      "Use smaller models (7B-8B parameters) for faster responses",
      "Adjust context length based on your needs (lower = faster)",
      "Keep LM Studio updated for performance improvements",
      "Monitor resource usage in LM Studio's performance tab"
    ],
    "useful_links": {
      "website": "https://lmstudio.ai",
      "documentation": "https://lmstudio.ai/docs",
      "discord": "https://discord.gg/lmstudio",
      "model_search": "Use the Discover tab in LM Studio"
    }
  }
}
